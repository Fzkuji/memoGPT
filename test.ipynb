{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:22:21.312483Z",
     "start_time": "2024-06-08T09:47:53.355426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir='/Volumes/Aurora/.cache/huggingface/hub',\n",
    ")"
   ],
   "id": "cdc0e9806e36990b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5676908ac6884aa18863a7531adaf85d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00004.safetensors:  55%|#####4    | 2.73G/4.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e97edefcfbe44f8793b17ba17f96bebc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "348d1755cc7a462199f0bb31305ef3cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d7843ca893d4acba8c90d708b098c0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb593ca9e5064c7e961187efea0eb975"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2140d302afb144859e5704c643bc8f29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3e58fceb3e74141a5734c85766b87ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:46:30.308946Z",
     "start_time": "2024-06-08T10:46:30.302014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import types\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "    outputs = self.model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        cache_position=cache_position,\n",
    "    )\n",
    "    print(attention_mask.shape)\n",
    "\n",
    "    hidden_states = outputs[0]\n",
    "    if self.config.pretraining_tp > 1:\n",
    "        lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "        logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "        logits = torch.cat(logits, dim=-1)\n",
    "    else:\n",
    "        logits = self.lm_head(hidden_states)\n",
    "    logits = logits.float()\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits,) + outputs[1:]\n",
    "        return (loss,) + output if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        past_key_values=outputs.past_key_values,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )\n",
    "\n",
    "model.model.forward = types.MethodType(forward, model)\n"
   ],
   "id": "e6f2835521c01f32",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:46:32.113269Z",
     "start_time": "2024-06-08T10:46:31.470377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ],
   "id": "7eb74cd5d22ab1d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 17\u001B[0m\n\u001B[1;32m      6\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[1;32m      7\u001B[0m     messages,\n\u001B[1;32m      8\u001B[0m     add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      9\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m )\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     12\u001B[0m terminators \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     13\u001B[0m     tokenizer\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[1;32m     14\u001B[0m     tokenizer\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<|eot_id|>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m ]\n\u001B[0;32m---> 17\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m     18\u001B[0m     input_ids,\n\u001B[1;32m     19\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m,\n\u001B[1;32m     20\u001B[0m     eos_token_id\u001B[38;5;241m=\u001B[39mterminators,\n\u001B[1;32m     21\u001B[0m     do_sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     22\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.6\u001B[39m,\n\u001B[1;32m     23\u001B[0m     top_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m,\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     25\u001B[0m response \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m][input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:]\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(response, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/memory/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/memory/lib/python3.11/site-packages/transformers/generation/utils.py:1591\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1588\u001B[0m     model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39muse_cache\n\u001B[1;32m   1590\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mand\u001B[39;00m requires_attention_mask \u001B[38;5;129;01mand\u001B[39;00m accepts_attention_mask:\n\u001B[0;32m-> 1591\u001B[0m     model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_attention_mask_for_generation(\n\u001B[1;32m   1592\u001B[0m         inputs_tensor, generation_config\u001B[38;5;241m.\u001B[39mpad_token_id, generation_config\u001B[38;5;241m.\u001B[39meos_token_id\n\u001B[1;32m   1593\u001B[0m     )\n\u001B[1;32m   1595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m model_kwargs:\n\u001B[1;32m   1596\u001B[0m     \u001B[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001B[39;00m\n\u001B[1;32m   1597\u001B[0m     model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_encoder_decoder_kwargs_for_generation(\n\u001B[1;32m   1598\u001B[0m         inputs_tensor, model_kwargs, model_input_name, generation_config\n\u001B[1;32m   1599\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/memory/lib/python3.11/site-packages/transformers/generation/utils.py:468\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_attention_mask_for_generation\u001B[0;34m(self, inputs, pad_token_id, eos_token_id)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;66;03m# Otherwise we have may have information -> try to infer the attention mask\u001B[39;00m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;66;03m# mps does not support torch.isin (https://github.com/pytorch/pytorch/issues/77764)\u001B[39;00m\n\u001B[0;32m--> 468\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    470\u001B[0m     )\n\u001B[1;32m    472\u001B[0m is_pad_token_in_inputs \u001B[38;5;241m=\u001B[39m (pad_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m    473\u001B[0m     torch\u001B[38;5;241m.\u001B[39misin(elements\u001B[38;5;241m=\u001B[39minputs, test_elements\u001B[38;5;241m=\u001B[39mpad_token_id)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m    474\u001B[0m )\n\u001B[1;32m    475\u001B[0m is_pad_token_not_equal_to_eos_token_id \u001B[38;5;241m=\u001B[39m (eos_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m~\u001B[39m(\n\u001B[1;32m    476\u001B[0m     torch\u001B[38;5;241m.\u001B[39misin(elements\u001B[38;5;241m=\u001B[39meos_token_id, test_elements\u001B[38;5;241m=\u001B[39mpad_token_id)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m    477\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9981565ccf1c5757"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
