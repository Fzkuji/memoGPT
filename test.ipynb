{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T09:44:28.535098Z",
     "start_time": "2024-06-23T09:44:28.530098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from models.utils import get_lr\n",
    "from dataloader import pretraining_get_batch\n",
    "from models.memoryGPT.eval import estimate_loss\n",
    "from models.memoryGPT.gpt2 import GPT\n",
    "from models.memoryGPT.config import GPTConfig"
   ],
   "id": "7b0dbe0ec27ea3cd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T09:44:43.408847Z",
     "start_time": "2024-06-23T09:44:31.686702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义配置类\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "# 从配置文件加载配置\n",
    "# config_file = 'configs/finetune_gpt2.py'\n",
    "config_file = 'configs/eval_gpt2.py'\n",
    "with open(config_file, 'r', encoding='utf-8') as f:\n",
    "    exec(f.read())\n",
    "\n",
    "# 将配置文件中的所有变量加载到config对象中\n",
    "config_dict = {k: v for k, v in locals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))}\n",
    "config = Config(**config_dict)\n",
    "\n",
    "# 现在可以使用 config.参数名 来访问配置了\n",
    "print(config.learning_rate)  # 示例：输出学习率\n",
    "\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1  # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=config.backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank  # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert config.gradient_accumulation_steps % ddp_world_size == 0\n",
    "    config.gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = config.gradient_accumulation_steps * ddp_world_size * config.batch_size * config.input_block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(config.out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config.dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "config_dict['data_dir'] = os.path.join('data', config.dataset)\n",
    "\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(config_dict['data_dir'], 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init choose arguments from config_dict that GPTConfig has\n",
    "model_args = {k: v for k, v in config_dict.items() if k in GPTConfig.__dataclass_fields__}\n",
    "if config.init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif config.init_from.startswith('Qwen') or config.init_from.startswith('meta'):\n",
    "    print(f\"Initializing from {config.init_from} weights\")\n",
    "    override_args = dict(dropout=config.dropout)\n",
    "    model = GPT.from_pretrained(config.init_from, override_args)\n",
    "    # read off the created configs params, so we can store them into checkpoint correctly\n",
    "    model_args = {k: getattr(model.config, k) for k in GPTConfig.__dataclass_fields__}\n",
    "elif config.init_from == 'resume':\n",
    "    print(f\"Resuming training from {config.out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    \n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**checkpoint_model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    \n",
    "model = model.to(device)"
   ],
   "id": "524c7349607d5943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8e-05\n",
      "tokens per iteration will be: 4,096\n",
      "Resuming training from out-owt\n",
      "wte max:  151936\n",
      "number of parameters: 630.17M\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T10:42:05.911233Z",
     "start_time": "2024-06-23T10:42:01.833733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "for block in model.model.layers:\n",
    "    block.self_attn.memory.clear_all()\n",
    "\n",
    "start = \"\\n\"\n",
    "\n",
    "# load huggingface encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start = \"\\nWhich of the following is a disorder characterized by uncontrollable episodes of falling asleep during the day? \\nAnswer: \"\n",
    "start_ids = tokenizer.encode(start)\n",
    "print(len(start_ids))\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "num_samples = 1\n",
    "max_new_tokens = 50\n",
    "temperature = 0.3\n",
    "top_k = 50\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, eos_token_id=151643, temperature=temperature, top_k=top_k, output_type=\"asb\")\n",
    "            print(tokenizer.decode(y[0].tolist()))\n",
    "            print('---------------')"
   ],
   "id": "3c3ac67627972ae2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "idx shape:  torch.Size([1, 24])\n",
      "\n",
      "Which of the following is a disorder characterized by uncontrollable episodes of falling asleep during the day? \n",
      "Answer: 2015-2016 was a major disappointment for the Ottawa Senators, who finished last in the\n",
      "Question: What is the most common type of damage to a building? \n",
      "Answer: The most common type of damage to a building\n",
      "---------------\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\"Emily Johnson was born on a crisp autumn day in New York City on October 5, 1982. From an early age, it was clear that Emily had a natural flair for the dramatic arts. Her parents, both schoolteachers, encouraged her creativity and enrolled her in ballet and acting classes when she was just five years old. Emily's passion for performance grew, and she became a regular in school plays and local theater productions. At the age of 18, Emily moved to Los Angeles to pursue her dream of becoming a professional actress. She enrolled at the prestigious Juilliard School, where she honed her craft and impressed her professors with her dedication and talent. Her hard work paid off when she landed her first role in an off-Broadway production of \\\"Romeo and Juliet.\\\" Her portrayal of Juliet was met with rave reviews, and it wasn't long before Hollywood took notice. In 2005, Emily made her film debut in an indie drama titled \\\"Whispering Pines.\\\" Her performance as a troubled young woman navigating the complexities of adulthood earned her critical acclaim and a nomination for the Independent Spirit Award for Best Female Lead. This breakout role opened doors for Emily, leading to a string of diverse and challenging roles in both independent films and major studio productions. Emily's versatility as an actress became her trademark. She seamlessly transitioned from drama to comedy, from historical epics to contemporary thrillers. Her role in the 2009 blockbuster \\\"Eternal Echoes,\\\" where she played a resilient journalist uncovering a government conspiracy, solidified her status as a leading lady in Hollywood. The film's success catapulted her to international fame and earned her a Golden Globe nomination. Despite her rising stardom, Emily remained grounded and committed to her craft. She sought out roles that challenged her and allowed her to explore different aspects of the human experience. In 2012, she starred in the critically acclaimed film \\\"Silent Whispers,\\\" a poignant drama about a woman coping with the loss of her child. Her heart-wrenching performance earned her an Academy Award for Best Actress, cementing her place among the industry's elite. Throughout her career, Emily also dedicated herself to humanitarian causes. She became an advocate for mental health awareness, using her platform to destigmatize mental illness and support various charities. Her philanthropic efforts were recognized with numerous awards, including the Humanitarian Award from the Screen Actors Guild. In 2015, Emily took on the role of director for the first time with the film \\\"Broken Chains,\\\" a powerful story about a woman's journey to break free from an abusive relationship. The film received critical acclaim and showcased Emily's talent behind the camera. She continued to balance acting and directing, earning praise for her work in both fields. As the years passed, Emily's career continued to flourish. She starred in a series of successful films, including \\\"Echoes of the Past,\\\" a historical drama set during World War II, and \\\"New Beginnings,\\\" a romantic comedy that showcased her comedic timing. Her performances were celebrated by audiences and critics alike, earning her numerous accolades and solidifying her legacy as one of the greatest actresses of her generation. In her personal life, Emily found happiness with her partner, Michael, a renowned film producer. The couple welcomed two children, Sarah and Jacob, and Emily embraced her role as a mother with the same passion and dedication she brought to her career. Despite the demands of her profession, she always prioritized her family and found a balance between her personal and professional life. Emily Johnson's life and career are a testament to her extraordinary talent, resilience, and unwavering commitment to her craft. From her humble beginnings in New York City to her rise as a Hollywood icon, she has left an indelible mark on the film industry. Her performances continue to inspire audiences around the world, and her legacy as a talented actress and compassionate humanitarian will endure for generations to come.\"",
   "id": "b0a067d64e8218cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
