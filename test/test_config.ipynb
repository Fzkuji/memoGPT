{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-18T03:18:07.714316Z",
     "start_time": "2024-06-18T03:18:07.710432Z"
    }
   },
   "source": [
    "import time\n",
    "from dataclasses import dataclass, field, fields\n",
    "import torch\n",
    "from models.memoryGPT.config import GPTConfig, TrainConfig\n",
    "\n",
    "\n",
    "# # 从配置文件加载配置\n",
    "# config_file = '../configs/finetune_gpt2.py'\n",
    "# with open(config_file, 'r', encoding='utf-8') as f:\n",
    "#     exec(f.read())\n",
    "# \n",
    "# # 将配置文件中的所有变量加载到config对象中\n",
    "# config_dict = {k: v for k, v in locals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))}\n",
    "# config = TrainConfig(**config_dict)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T03:18:08.024331Z",
     "start_time": "2024-06-18T03:18:08.018417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从配置文件加载配置\n",
    "config_file = '../configs/finetune_gpt2.py'\n",
    "config_vars = {}\n",
    "with open(config_file, 'r', encoding='utf-8') as f:\n",
    "    exec(f.read(), {}, config_vars)\n",
    "\n",
    "# 将配置文件中的所有变量加载到config对象中\n",
    "config_dict = {k: v for k, v in config_vars.items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))}\n",
    "train_config_fields = {field.name for field in fields(TrainConfig)}\n",
    "filtered_config_dict = {k: v for k, v in config_dict.items() if k in train_config_fields}\n",
    "config = TrainConfig(**filtered_config_dict)\n"
   ],
   "id": "b9e052f614c31b0c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T03:18:08.540196Z",
     "start_time": "2024-06-18T03:18:08.535064Z"
    }
   },
   "cell_type": "code",
   "source": "config_dict",
   "id": "ee19f5961fcb98e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_dir': 'out-owt',\n",
       " 'eval_interval': 500,\n",
       " 'eval_iters': 100,\n",
       " 'eval_only': False,\n",
       " 'log_interval': 10,\n",
       " 'wandb_log': False,\n",
       " 'wandb_project': 'owt',\n",
       " 'wandb_run_name': 'ft-1718680688.0224152',\n",
       " 'dataset': 'fineweb',\n",
       " 'train_mode': 'pretrain',\n",
       " 'init_from': 'resume',\n",
       " 'always_save_checkpoint': False,\n",
       " 'batch_size': 1,\n",
       " 'gradient_accumulation_steps': 16,\n",
       " 'max_iters': 600000,\n",
       " 'lr_decay_iters': 100000,\n",
       " 'warmup_iters': 200,\n",
       " 'memory_dim': 896,\n",
       " 'intermediate_size': 4864,\n",
       " 'n_layer': 24,\n",
       " 'n_embd': 896,\n",
       " 'num_attention_heads': 14,\n",
       " 'num_key_value_heads': 2,\n",
       " 'short_term_memory_size': 16,\n",
       " 'bias': True,\n",
       " 'rms_norm_eps': 1e-06,\n",
       " 'block_size': 1024,\n",
       " 'input_block_size': 256,\n",
       " 'train_size_ratio': 16,\n",
       " 'val_size_ratio': 256,\n",
       " 'train_size': 4096,\n",
       " 'val_size': 65536,\n",
       " 'learning_rate': 8e-05,\n",
       " 'decay_lr': True,\n",
       " 'min_lr': 1e-06,\n",
       " 'use_moe': False,\n",
       " 'n_expert': 16,\n",
       " 'n_expert_per_tok': 4,\n",
       " 'dropout': 0.0,\n",
       " 'weight_decay': 0.1,\n",
       " 'beta1': 0.9,\n",
       " 'beta2': 0.95,\n",
       " 'grad_clip': 1.0,\n",
       " 'backend': 'nccl',\n",
       " 'device': 'cuda',\n",
       " 'dtype': 'bfloat16',\n",
       " 'compile': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T04:02:06.923582Z",
     "start_time": "2024-06-18T04:01:56.484803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import fields\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from models.utils import get_lr\n",
    "from dataloader import get_batch, CustomDataset, collate_fn\n",
    "from models.memoryGPT.eval import estimate_loss\n",
    "from models.memoryGPT.gpt2 import GPT\n",
    "from models.memoryGPT.config import GPTConfig, TrainConfig\n",
    "\n",
    "\n",
    "# # 从配置文件加载配置\n",
    "# config_file = 'configs/finetune_gpt2.py'\n",
    "# with open(config_file, 'r', encoding='utf-8') as f:\n",
    "#     exec(f.read())\n",
    "#\n",
    "# # 将配置文件中的所有变量加载到config对象中\n",
    "# config_dict = {k: v for k, v in locals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))}\n",
    "# config = TrainConfig(**config_dict)\n",
    "\n",
    "# 从配置文件加载配置\n",
    "config_file = '../configs/finetune_gpt2.py'\n",
    "config_vars = {}\n",
    "with open(config_file, 'r', encoding='utf-8') as f:\n",
    "    exec(f.read(), {}, config_vars)\n",
    "\n",
    "# 将配置文件中的所有变量加载到config对象中\n",
    "config_dict = {k: v for k, v in config_vars.items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))}\n",
    "train_config_fields = {field.name for field in fields(TrainConfig)}\n",
    "filtered_config_dict = {k: v for k, v in config_dict.items() if k in train_config_fields}\n",
    "config = TrainConfig(**filtered_config_dict)\n",
    "\n",
    "# 现在可以使用 config.参数名 来访问配置了\n",
    "print(config)\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1  # is this a ddp run?\n",
    "if ddp:\n",
    "    print(\"using distributed data parallel\")\n",
    "    init_process_group(backend=config.backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank  # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert config.gradient_accumulation_steps % ddp_world_size == 0\n",
    "    config.gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    print(\"not using distributed data parallel\")\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = config.gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(config.out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config.dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "config_dict['data_dir'] = os.path.join('data', config.dataset)\n",
    "print(f\"load data from {config_dict['data_dir']}\")\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(config_dict['data_dir'], 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init choose arguments from config_dict that GPTConfig has\n",
    "model_args = {k: v for k, v in config_dict.items() if k in GPTConfig.__dataclass_fields__}\n",
    "if config.init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif config.init_from.startswith('Qwen') or config.init_from.startswith('meta'):\n",
    "    print(f\"Initializing from {config.init_from} weights\")\n",
    "    override_args = dict(dropout=config.dropout)\n",
    "    model = GPT.from_pretrained(config.init_from, override_args)\n",
    "    # read off the created configs params, so we can store them into checkpoint correctly\n",
    "    model_args = {k: getattr(model.config, k) for k in GPTConfig.__dataclass_fields__}\n",
    "elif config.init_from == 'resume':\n",
    "    print(f\"Resuming training from {config.out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "\n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**checkpoint_model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']"
   ],
   "id": "54cda777d1065a23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainConfig(max_batch_size=64, short_term_memory_size=16, long_term_memory_layer=16, long_term_memory_chunk_size=4, rope_theta=500000, rms_norm_eps=1e-06, block_size=1024, input_block_size=256, vocab_size=50304, n_layer=24, num_attention_heads=14, num_key_value_heads=2, use_moe=False, n_expert=16, n_expert_per_tok=4, n_embd=896, intermediate_size=4864, dropout=0.0, bias=True, device='cuda', init_from='Qwen/Qwen2-0.5B-Instruct', config_file='configs/finetune_gpt2.py', out_dir='out-owt', eval_interval=500, eval_iters=100, eval_only=False, log_interval=10, wandb_log=False, wandb_project='owt', wandb_run_name='ft-1718683316.4971983', dataset='fineweb', train_mode='pretrain', always_save_checkpoint=False, train_size_ratio=16, val_size_ratio=256, train_size=4096, val_size=65536, batch_size=1, gradient_accumulation_steps=16, max_iters=600000, lr_decay_iters=100000, warmup_iters=200, memory_dim=896, learning_rate=8e-05, decay_lr=True, min_lr=1e-06, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, backend='nccl', dtype='bfloat16', compile=True)\n",
      "not using distributed data parallel\n",
      "tokens per iteration will be: 16,384\n",
      "load data from data\\fineweb\n",
      "Initializing from Qwen/Qwen2-0.5B-Instruct weights\n",
      "loading weights from pretrained Qwen2: Qwen/Qwen2-0.5B-Instruct\n",
      "forcing block_size=32768, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "wte max:  151936\n",
      "number of parameters: 630.17M\n",
      "loaded successfully\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
