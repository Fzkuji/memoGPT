{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T03:38:10.653628Z",
     "start_time": "2024-06-16T03:38:04.891303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")"
   ],
   "id": "e99da237a6ec777d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T03:39:34.481685Z",
     "start_time": "2024-06-16T03:39:34.474084Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.forward)",
   "id": "c3aac83a3e3336d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Qwen2ForCausalLM.forward of Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm()\n",
      "        (post_attention_layernorm): Qwen2RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T03:27:29.940767Z",
     "start_time": "2024-06-16T03:27:23.207940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T03:27:29.946561Z",
     "start_time": "2024-06-16T03:27:29.942535Z"
    }
   },
   "cell_type": "code",
   "source": "print(response)",
   "id": "2cf7a5810f4e3dcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a machine learning model that can generate human-like text based on input sentences or prompts. These models are trained using large amounts of data, which means they have access to vast amounts of information and can generate output with high accuracy and coherence.\n",
      "\n",
      "Large language models are often used in natural language processing tasks such as chatbots, speech recognition, and question answering. They can also be used for tasks such as language translation, summarization, and recommendation systems.\n",
      "\n",
      "In addition to generating human-like text, large language models can also process and analyze large volumes of data, making them useful for various applications in fields such as finance, healthcare, and security.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:40:57.659155Z",
     "start_time": "2024-06-14T15:40:57.649627Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.state_dict().keys())",
   "id": "55d6eb5851227cd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:40:58.031183Z",
     "start_time": "2024-06-14T15:40:57.660852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print all state_dict shape\n",
    "for key in model.state_dict().keys():\n",
    "    print(key, model.state_dict()[key].shape)"
   ],
   "id": "ecd6a1e91d1b4d7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([151936, 896])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.0.input_layernorm.weight torch.Size([896])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.1.input_layernorm.weight torch.Size([896])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.2.input_layernorm.weight torch.Size([896])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.3.input_layernorm.weight torch.Size([896])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.4.input_layernorm.weight torch.Size([896])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.5.input_layernorm.weight torch.Size([896])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.6.input_layernorm.weight torch.Size([896])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.7.input_layernorm.weight torch.Size([896])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.8.input_layernorm.weight torch.Size([896])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.9.input_layernorm.weight torch.Size([896])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.10.input_layernorm.weight torch.Size([896])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.11.input_layernorm.weight torch.Size([896])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.12.input_layernorm.weight torch.Size([896])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.13.input_layernorm.weight torch.Size([896])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.14.input_layernorm.weight torch.Size([896])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.15.input_layernorm.weight torch.Size([896])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.16.input_layernorm.weight torch.Size([896])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.17.input_layernorm.weight torch.Size([896])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.18.input_layernorm.weight torch.Size([896])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.19.input_layernorm.weight torch.Size([896])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.20.input_layernorm.weight torch.Size([896])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.21.input_layernorm.weight torch.Size([896])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.22.input_layernorm.weight torch.Size([896])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.23.input_layernorm.weight torch.Size([896])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([896])\n",
      "model.norm.weight torch.Size([896])\n",
      "lm_head.weight torch.Size([151936, 896])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:10:06.508028Z",
     "start_time": "2024-06-15T14:46:40.108375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "\n"
   ],
   "id": "2cabc64ab9184ff4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1208ebc8c7944be2a1fe79eafdf11f6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fzkuj\\anaconda3\\envs\\memory\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fzkuj\\.cache\\huggingface\\hub\\models--Qwen--Qwen2-7B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ddb2a3d967848fb8562be3c145b5c7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b33fe75d5aa41e9b681ec1af57ad14b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4856c0c85ab24568ace9baa8db052aed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37dacd6447cc412d81cc9608d488fc6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05b336272ae3495fabd1800c1f3cae1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fe274e5d1c2491399ba46d6abe9624b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5009e58463c240a6966cea1108b890cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "410da46f01264cb2bbb3517f5f0ccb33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ab00e3a955e46c8bbdd88d4d18ce3ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aeed4b7d04594a0aa3927be571343407"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2f85ed35c7947f9971d72a81a4b51aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "190e819cbefe4398a6ae8739e0cfd0dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:14:06.588358Z",
     "start_time": "2024-06-15T15:13:29.422624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Mary moved to the bathroom. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable.Released in January 2011 in Japan, it is the third game in the Valkyria series.Employing the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \\\"Nameless\\\", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \\\" Calamaty Raven \\\". The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II.While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers.Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa.The game's opening theme was sung by May 'n. It met with positive sales in Japan, and was praised by both Japanese and western critics.After release, it received downloadable content, along with an expanded edition in November of that year.It was also adapted into manga and an original video animation series.Due to low sales of Valkyria Chronicles II, Valkyria Chronicles III was not localized, but a fan translation compatible with the game's expanded edition was released in 2014.Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4.John went to the hallway. As with previous Valkyira Chronicles games, Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces.Stories are told through comic book @-@ like panels with animated character portraits, with characters speaking partially through voiced speech bubbles and partially through unvoiced text.The player progresses through a series of linear missions, gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked.The route to each story location on the map varies depending on an individual player's approach : when one option is selected, the other is sealed. \\nQuestion: Where is Mary?\\nAnswer:\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "response"
   ],
   "id": "db7367873debff07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary is in the bathroom.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3d5cad79c42ceac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
